{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8e63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2698c6",
   "metadata": {},
   "source": [
    "# Creating positive and negative words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ba5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_values_in_dict(word_dict,key, list_of_words):\n",
    "    if key not in word_dict:\n",
    "        word_dict[key] = list()\n",
    "    word_dict[key].append(list_of_words)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24cf4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {'Positive':[], 'Negative':[]} #creating dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6590f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_filtered = \"D:/Text_analysis/Filtered_data\"\n",
    "path = 'D:/Text_analysis/MasterDictionary-20230305T133348Z-001/MasterDictionary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c77599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting positive and negative words lists\n",
    "positive = []\n",
    "negative = []\n",
    "for filename in os.listdir(path):\n",
    "    f = os.path.join(path, filename)\n",
    "    with open(f, 'r', encoding='latin-1') as file1:\n",
    "        if filename == 'positive-words.txt':\n",
    "              positive = (file1.read().split())\n",
    "        elif filename == 'negative-words.txt':\n",
    "              negative = (file1.read().split())\n",
    "        else:\n",
    "              continue          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3af37a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:05<00:00, 18.95it/s]\n"
     ]
    }
   ],
   "source": [
    "for filename in tqdm(os.listdir(path_filtered)):\n",
    "    f = os.path.join(path_filtered, filename)\n",
    "    if os.path.isfile(f):\n",
    "        with open(f,encoding='latin-1') as file1:\n",
    "            words = file1.read().split()\n",
    "            for r in words:\n",
    "                if r in positive:\n",
    "                    word_dict = add_values_in_dict(word_dict, 'Positive', r)\n",
    "                elif r in negative:\n",
    "                    word_dict = add_values_in_dict(word_dict, 'Negative', r)\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e0602f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('D:\\Text_analysis\\Dictionary\\dictionary.json', 'w')\n",
    "json.dump(word_dict, file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7ac18",
   "metadata": {},
   "source": [
    "# Word and Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26cd21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 111/111 [00:00<00:00, 167.28it/s]\n"
     ]
    }
   ],
   "source": [
    "path_dest='D:/Text_analysis/Tokens'\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "for filename in tqdm(os.listdir(path_filtered)):\n",
    "    f = os.path.join(path_filtered, filename)\n",
    "    with open(f,encoding='latin-1') as file1:\n",
    "        text = file1.read()\n",
    "        file1.close()\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    sent_tokens = sent_tokenize(text)\n",
    "  \n",
    "    word_destination = path_dest + '/word_tokens/wt_' + base +'.json'\n",
    "    sent_destination = path_dest + '/sent_tokens/st_' + base +'.json'\n",
    "  \n",
    "    with open(word_destination, 'w') as a:\n",
    "        json.dump(word_tokens, a)\n",
    "    a.close()\n",
    "    with open(sent_destination,'w') as a:\n",
    "        json.dump(sent_tokens, a)\n",
    "    a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af04864",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict=\"D:\\Text_analysis\\Dictionary\\dictionary.json\"\n",
    "dictionary = open(path_dict, 'r')\n",
    "word_dict = json.load(dictionary)\n",
    "dictionary.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298c72d",
   "metadata": {},
   "source": [
    "# Extracting derived variables:\n",
    "## Positive Score: \n",
    "This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
    "\n",
    "## Negative Score: \n",
    "This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n",
    "\n",
    "## Polarity Score: \n",
    "This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula:\n",
    "\n",
    "## Polarity Score = \n",
    "(Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001) Range is from -1 to +1\n",
    "\n",
    "## Subjectivity Score: \n",
    "This is the score that determines if a given text is objective or subjective. It is calculated by using the formula:\n",
    "\n",
    "## Subjectivity Score = \n",
    "(Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001) Range is from 0 to +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbfde71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_wt='D:/Text_analysis/Tokens/word_tokens'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eaf3350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:06<00:00, 17.77it/s]\n"
     ]
    }
   ],
   "source": [
    "p_score = []\n",
    "n_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "url_ids = []\n",
    "\n",
    "\n",
    "for filename in tqdm(os.listdir(path_wt)):\n",
    "    f = os.path.join(path_wt, filename)\n",
    "    file1 = open(f, 'r')\n",
    "    words = json.load(file1)\n",
    "    p = 0\n",
    "    n = 0\n",
    "    polar_score = 0\n",
    "    no_of_words = len(words)\n",
    "#  print(no_of_words, filename)\n",
    "    for word in words:\n",
    "        if word in word_dict['Positive']:\n",
    "              p += 1\n",
    "        elif word in word_dict['Negative']:\n",
    "              n += 1\n",
    "        else:\n",
    "              continue\n",
    "  #print('p',(p - n)/((p + n) + 0.000001))\n",
    "    url_ids.append(re.findall(\"\\d+\", filename))\n",
    "    flat_list = []\n",
    "    p_score.append(p)\n",
    "    n_score.append(n)\n",
    "    polarity_score.append((p - n)/((p + n) + 0.000001))\n",
    "    subjectivity_score.append((p + n)/((no_of_words) + 0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ad34905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list(list_2d):\n",
    "    flat_list = []\n",
    "    for sublist in list_2d:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "flat_list1 = flat_list(url_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86c5862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = {'url_id': flat_list1, 'p_score': p_score, 'n_score': n_score, \n",
    "         'polarity_score': polarity_score,\n",
    "         'subjectivity_score': subjectivity_score}\n",
    "file1 = open('D:/Text_analysis/results/scores.json', 'w')\n",
    "json.dump(score, file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba3c10",
   "metadata": {},
   "source": [
    "## Analysis of Readability\n",
    "It is calculated using the Gunning Fox index formula described below:\n",
    "\n",
    "Average Sentence Length = the number of words / the number of sentences\n",
    "\n",
    "Percentage of Complex words = the number of complex words / the number of words\n",
    "\n",
    "Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8e627cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_path='D:\\Text_analysis\\Tokens\\word_tokens'\n",
    "sent_path='D:\\Text_analysis\\Tokens\\sent_tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a6b9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.listdir(word_path), os.listdir(sent_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "775a8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_complex_words(words_list):\n",
    "    c = 0\n",
    "    for word in words_list:\n",
    "        l = re.findall('(?!e$)[aeiou]+', word, re.I)+re.findall('^[aeiouy]*e$', word, re.I)\n",
    "        if len(l) > 2:\n",
    "            c += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fba9f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 111/111 [00:00<00:00, 308.05it/s]\n"
     ]
    }
   ],
   "source": [
    "avg_sent_length = []\n",
    "percent_of_complex_words = []\n",
    "fog_index = []\n",
    "url_id = []\n",
    "\n",
    "for i in tqdm(range(len(files[0]))):\n",
    "    w = os.path.join(word_path, files[0][i])\n",
    "    s = os.path.join(sent_path, files[1][i])\n",
    "\n",
    "    file1 = open(w, 'r')\n",
    "    words = json.load(file1)\n",
    "    file1.close()\n",
    "    no_of_words = len(words)\n",
    "\n",
    "    file2 = open(s, 'r')\n",
    "    sent = json.load(file2)\n",
    "    file2.close()\n",
    "\n",
    "    url_id.append(re.findall(\"\\d+\", files[0][i]))\n",
    "    avg_sent_length.append(int(no_of_words/len(sent)))\n",
    "    percent_of_complex_words.append(count_complex_words(words)/no_of_words)\n",
    "    fog_index.append(0.4*(avg_sent_length[i]+percent_of_complex_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca8d486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list2 = flat_list(url_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5f67ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "readability = {'url_id': flat_list2, 'avg_sent_length': avg_sent_length,\n",
    "               'percent_of_complex_words': percent_of_complex_words,\n",
    "               'fog_index': fog_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e14412e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('D:/Text_analysis/results/readability_analysis.json', 'w')\n",
    "json.dump(readability, file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14163a4c",
   "metadata": {},
   "source": [
    "## Average Number of Words Per Sentence\n",
    "The formula for calculating is:\n",
    "\n",
    "Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
    "\n",
    "Complex Word Count\n",
    "Complex words are words in the text that contain more than two syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53d0b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_extracted = 'D:/Text_analysis/Filtered_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "456ee501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 111/111 [00:00<00:00, 137.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "avg_no_words_per_sent = []\n",
    "complex_word_count = []\n",
    "url_id = []\n",
    "\n",
    "for filename in tqdm(os.listdir(path_extracted)):\n",
    "    f = os.path.join(path_extracted, filename)\n",
    "    with open(f, encoding='latin-1') as file1:\n",
    "        text = file1.read()\n",
    "        file1.close()\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        word_tokens = word_tokenize(text)\n",
    "        sent_tokens = sent_tokenize(text)\n",
    "\n",
    "        url_id.append(base)\n",
    "        avg_no_words_per_sent.append(int(len(word_tokens)/len(sent_tokens)))\n",
    "        complex_word_count.append(count_complex_words(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "637a6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_words_and_complex_words = {'url_id': url_id, 'avg_no_words_per_sent': avg_no_words_per_sent,\n",
    "                               'compplex_word_count': complex_word_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcdf44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('D:/Text_analysis/results/avg_and_complex_words.json', 'w')\n",
    "json.dump(avg_words_and_complex_words, file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d45b0c",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "We count the total cleaned words present in the text by\n",
    "\n",
    "removing the stop words (using stopwords class of nltk package).\n",
    "removing any punctuations like ? ! , . from the word before counting.\n",
    "Syllable Count Per Word\n",
    "We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c505feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    c = 0\n",
    "    vowels = 'aeiou'\n",
    "    l = re.findall(f'(?!e$)(?!es$)(?!ed$)[{vowels}]', word, re.I)\n",
    "    return len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aef9aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 111/111 [00:00<00:00, 242.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# using RegexpTokenizer to remove punctuations\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "url_id = []\n",
    "word_count = []\n",
    "syllable_count = []\n",
    "\n",
    "for filename in tqdm(os.listdir(path_extracted)):\n",
    "    f = os.path.join(path_extracted, filename)\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    with open(f,encoding='latin-1') as file1:\n",
    "        text = file1.read()\n",
    "        file1.close()\n",
    "  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = tokenizer.tokenize(text)\n",
    "    filtered_words = []\n",
    "    syllable_per_word = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "            list1 = []\n",
    "            list1.append(word)\n",
    "            list1.append(count_syllables(word))\n",
    "            syllable_per_word.append(list1)\n",
    "\n",
    "    path = 'D:/Text_analysis/results/syllable_per_word' + '/' + base + '_syllable_count_per_word.json'\n",
    "    file1 = open(path, 'w')\n",
    "    json.dump(syllable_per_word, file1)\n",
    "    file1.close()\n",
    "\n",
    "    url_id.append(base)\n",
    "    word_count.append(len(filtered_words))\n",
    "    syllable_sum = 0\n",
    "    for i in range(len(syllable_per_word)):\n",
    "        syllable_sum += syllable_per_word[i][1]\n",
    "    syllable_count.append(int(syllable_sum/len(syllable_per_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4225359",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_and_syllable_count = {'url_id': url_id, 'word_count': word_count, 'syllable_count': syllable_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3d02c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('D:/Text_analysis/results/word_and_syllable_count.json','w')\n",
    "json.dump(word_and_syllable_count, file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a71bc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    pronoun_count = re.compile(r'\\b(I|we|ours|my|mine|(?-i:us))\\b', re.I)\n",
    "    pronouns = pronoun_count.findall(text)\n",
    "    return len(pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ced6c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 111/111 [00:00<00:00, 2174.47it/s]\n"
     ]
    }
   ],
   "source": [
    "url_id = []\n",
    "personal_pronouns_count = []\n",
    "word_avg_length = []\n",
    "\n",
    "for filename in tqdm(os.listdir(path_extracted)):\n",
    "    f = os.path.join(path_extracted, filename)\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    with open(f,encoding='latin-1') as file1:\n",
    "        text = file1.read()\n",
    "        words = text.split()\n",
    "        file1.close()\n",
    "  \n",
    "    c = 0\n",
    "    for word in words:\n",
    "        c += len(word)\n",
    "    \n",
    "    url_id.append(base)\n",
    "    personal_pronouns_count.append(count_personal_pronouns(text))\n",
    "    word_avg_length.append(round(c/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e75f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns_and_word_length = {'url_id': url_id, 'personal_pronouns': personal_pronouns_count,\n",
    "                            'word_avg_length': word_avg_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "366b5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Text_analysis/results/pronouns_and_word_length.json', 'w') as file1:\n",
    "    json.dump(pronouns_and_word_length, file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a763bc",
   "metadata": {},
   "source": [
    "## Collecting all the results and saving in the excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f794090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_result = 'D:/Text_analysis/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a39e1423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 1997.76it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for filename in tqdm(os.listdir(path_result)):\n",
    "    f = os.path.join(path_result, filename)\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    try:\n",
    "        with open(f, 'r') as file1:\n",
    "            results[base] = json.load(file1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc8b3673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_and_complex_words\n",
      "pronouns_and_word_length\n",
      "readability_analysis\n",
      "scores\n",
      "word_and_syllable_count\n"
     ]
    }
   ],
   "source": [
    "df = {}\n",
    "for key in results.keys():\n",
    "    print(key)\n",
    "    df[key] = pd.DataFrame.from_dict(results[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "374afa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(df['scores'], df['readability_analysis'], on ='url_id', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88715222",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df1 = pd.merge(df['avg_and_complex_words'], df['word_and_syllable_count'],\n",
    "                  on = 'url_id',how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b92faa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df1 = pd.merge(all_df1, df['pronouns_and_word_length'], on='url_id', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecf3b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(all_df, all_df1, on = 'url_id', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11a79576",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df.astype({'url_id': float})\n",
    "all_df.sort_values('url_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f585ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 = list(all_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4185eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "db= pd.read_excel(\"D:\\Text_analysis\\Output Data Structure.xlsx\")\n",
    "db.set_index(['URL_ID'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "011d5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_working_url_id=[44,57,122]\n",
    "db.drop(not_working_url_id, axis = 0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48b89d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.to_excel('D:/Text_analysis/new Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a5db3",
   "metadata": {},
   "source": [
    "## Importing output structure sheet and saving final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2275154e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112     149  https://insights.blackcoffer.com/business-anal...   \n",
       "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0               NaN             NaN             NaN                 NaN   \n",
       "1               NaN             NaN             NaN                 NaN   \n",
       "2               NaN             NaN             NaN                 NaN   \n",
       "3               NaN             NaN             NaN                 NaN   \n",
       "4               NaN             NaN             NaN                 NaN   \n",
       "..              ...             ...             ...                 ...   \n",
       "109             NaN             NaN             NaN                 NaN   \n",
       "110             NaN             NaN             NaN                 NaN   \n",
       "111             NaN             NaN             NaN                 NaN   \n",
       "112             NaN             NaN             NaN                 NaN   \n",
       "113             NaN             NaN             NaN                 NaN   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                    NaN                          NaN        NaN   \n",
       "1                    NaN                          NaN        NaN   \n",
       "2                    NaN                          NaN        NaN   \n",
       "3                    NaN                          NaN        NaN   \n",
       "4                    NaN                          NaN        NaN   \n",
       "..                   ...                          ...        ...   \n",
       "109                  NaN                          NaN        NaN   \n",
       "110                  NaN                          NaN        NaN   \n",
       "111                  NaN                          NaN        NaN   \n",
       "112                  NaN                          NaN        NaN   \n",
       "113                  NaN                          NaN        NaN   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                                 NaN                 NaN         NaN   \n",
       "1                                 NaN                 NaN         NaN   \n",
       "2                                 NaN                 NaN         NaN   \n",
       "3                                 NaN                 NaN         NaN   \n",
       "4                                 NaN                 NaN         NaN   \n",
       "..                                ...                 ...         ...   \n",
       "109                               NaN                 NaN         NaN   \n",
       "110                               NaN                 NaN         NaN   \n",
       "111                               NaN                 NaN         NaN   \n",
       "112                               NaN                 NaN         NaN   \n",
       "113                               NaN                 NaN         NaN   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                  NaN                NaN              NaN  \n",
       "1                  NaN                NaN              NaN  \n",
       "2                  NaN                NaN              NaN  \n",
       "3                  NaN                NaN              NaN  \n",
       "4                  NaN                NaN              NaN  \n",
       "..                 ...                ...              ...  \n",
       "109                NaN                NaN              NaN  \n",
       "110                NaN                NaN              NaN  \n",
       "111                NaN                NaN              NaN  \n",
       "112                NaN                NaN              NaN  \n",
       "113                NaN                NaN              NaN  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.read_excel(\"D:\\Text_analysis\\Output Data Structure.xlsx\")\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb8af972",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(main_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80ba4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.remove('URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5cb9382",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [column1, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29fc5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_dict = {}\n",
    "for i in range(len(column_list[0])):\n",
    "    column_dict[column_list[0][i]] = column_list[1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9743a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.rename(columns=column_dict, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f5b10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(main_df,all_df, on='URL_ID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2822ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['POSITIVE SCORE_x', 'NEGATIVE SCORE_x',\n",
    "       'POLARITY SCORE_x', 'SUBJECTIVITY SCORE_x', 'AVG SENTENCE LENGTH_x',\n",
    "       'PERCENTAGE OF COMPLEX WORDS_x', 'FOG INDEX_x',\n",
    "       'AVG NUMBER OF WORDS PER SENTENCE_x', 'COMPLEX WORD COUNT_x',\n",
    "       'WORD COUNT_x', 'SYLLABLE PER WORD_x', 'PERSONAL PRONOUNS_x',\n",
    "       'AVG WORD LENGTH_x'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3cbaafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('D:/Text_analysis/results/Final_Result.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
